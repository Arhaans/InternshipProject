from transformers import AutoProcessor, AutoModelForImageTextToText
from PIL import Image
import requests
import torch
import os
import sys # For sys.exit()

# --- Configuration ---
MODEL_ID = "google/medgemma-4b-it"
LOCAL_IMAGE_FILENAME = "xray_sample.jpg"
FALLBACK_IMAGE_URL = "https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png" # A public domain chest X-ray
MAX_NEW_TOKENS = 256 # Increase max_new_tokens for potentially more detailed responses

# --- 1. Load Model and Processor ---
print("Loading model and processor...")
try:
    # Use torch.bfloat16 for better performance and memory efficiency, as recommended
    # device_map="auto" intelligently distributes the model across available devices (e.g., GPU if available)
    model = AutoModelForImageTextToText.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    processor = AutoProcessor.from_pretrained(MODEL_ID)
    print("Model and processor loaded successfully.")
except Exception as e:
    print(f"Error loading model or processor: {e}")
    print("Please ensure you have 'transformers', 'accelerate', 'pillow', 'requests', and 'torch' installed.")
    sys.exit(1) # Exit if model cannot be loaded

# --- 2. Load the Image ---
image = None
if os.path.exists(LOCAL_IMAGE_FILENAME):
    print(f"Opening local image: {LOCAL_IMAGE_FILENAME}")
    try:
        image = Image.open(LOCAL_IMAGE_FILENAME).convert("RGB")
    except Exception as e:
        print(f"Error opening local image '{LOCAL_IMAGE_FILENAME}': {e}")
        print("Attempting to download fallback image.")

if image is None: # If local image failed or didn't exist
    print(f"Local image '{LOCAL_IMAGE_FILENAME}' not found or could not be opened. Attempting to download a sample image.")
    try:
        # Use a User-Agent header to avoid potential HTTP 403 errors from some servers
        response = requests.get(FALLBACK_IMAGE_URL, headers={"User-Agent": "Mozilla/5.0"}, stream=True)
        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)
        image = Image.open(response.raw).convert("RGB")
        print("Sample image downloaded and opened successfully.")
    except requests.exceptions.RequestException as e:
        print(f"Error downloading sample image from URL: {e}")
        print("Please ensure you have a working internet connection.")
        sys.exit(1)
    except Exception as e:
        print(f"Error processing downloaded image: {e}")
        sys.exit(1)

# --- 3. Construct Messages in Chat Format ---
# This structured format is crucial for multimodal models like MedGemma.
# The system prompt is expanded based on recommendations from the MedSight article
# to guide the model to generate detailed and clinically useful descriptions.
messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a highly experienced and accurate medical imaging AI. Your task is to provide clear, clinically useful descriptions of medical images. Identify relevant anatomical structures, patterns, anomalies, and potential diagnoses. Focus on the location and characteristics of abnormalities, indicators of common pathologies, and whether the image appears normal or not. Use formal, clinical language. Do not add disclaimers unless findings are uncertain. If the image quality is poor, state this clearly. Start your response with a short summary followed by a more detailed paragraph."}]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "Describe this X-ray in detail, including any findings."}, # A slightly more directive user prompt
            {"type": "image", "image": image}
        ]
    }
]

# --- 4. Preprocess Inputs ---
print("Applying chat template and preparing inputs...")
try:
    inputs = processor.apply_chat_template(
        messages,
        add_generation_prompt=True, # Important for conversational models to generate a response
        tokenize=True,              # Tokenize the input messages
        return_dict=True,           # Return a dictionary of inputs
        return_tensors="pt"         # Return PyTorch tensors
    ).to(model.device, dtype=torch.bfloat16) # Move inputs to the correct device and data type
    print("Inputs prepared successfully.")
except Exception as e:
    print(f"Error preparing inputs: {e}")
    sys.exit(1)

# Get the length of the input tokens. This is used to slice the generated output
# so we only decode the *new* tokens generated by the model.
input_len = inputs["input_ids"].shape[-1]

# --- 5. Generate Output ---
print("Generating output...")
try:
    with torch.inference_mode(): # Use torch.inference_mode() for efficient inference without tracking gradients
        generation = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS, # Set the maximum number of new tokens to generate
            do_sample=True,     # Enable sampling for potentially more diverse/coherent output
            top_p=0.9,          # Nucleus sampling parameter (recommended for text generation)
            temperature=0.7,    # Controls randomness of generation (adjust for creativity vs. determinism)
            eos_token_id=processor.tokenizer.eos_token_id, # Ensure generation stops at EOS token
            pad_token_id=processor.tokenizer.pad_token_id # Ensure pad token is recognized
        )
        
        # Slice the generated tokens to only get the new output from the model
        # This removes the input prompt tokens from the generated sequence
        generated_tokens = generation[0, input_len:] # Take the first (and likely only) sequence
        
        # Decode the generated tokens, skipping special tokens for clean output
        output_text = processor.decode(generated_tokens, skip_special_tokens=True) 
        
    print("Output generation complete.")
except Exception as e:
    print(f"Error during generation: {e}")
    sys.exit(1)

# --- 6. Display Result ---
print("\n--- Model Response ---")
if output_text.strip(): # Check if the decoded output is not empty or just whitespace
    print(output_text.strip())
else:
    # If the output is still empty, print the raw tokens for further debugging
    raw_output_tokens = processor.decode(generated_tokens, skip_special_tokens=False)
    print("No meaningful text generated. Raw tokens (for debugging):")
    print(raw_output_tokens)
    print("\nTroubleshooting tips:")
    print("- Try adjusting 'temperature' and 'top_p' parameters in model.generate().")
    print(f"- Increase MAX_NEW_TOKENS (currently {MAX_NEW_TOKENS}).")
    print("- Ensure the input image is clear and relevant to medical imaging.")
